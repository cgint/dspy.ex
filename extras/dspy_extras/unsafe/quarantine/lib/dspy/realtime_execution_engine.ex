defmodule Dspy.RealtimeExecutionEngine do
  @moduledoc """
  Realtime execution engine for continuous DSPy operations.

  This engine enables truly realtime, ongoing execution of DSPy examples and experiments
  with configurable update intervals, streaming results, and live monitoring.

  ## Features

  - Continuous execution with configurable intervals
  - Real-time result streaming via Phoenix LiveView or WebSockets
  - Live metrics and performance monitoring
  - Adaptive execution based on performance feedback
  - Fault tolerance and automatic recovery
  - Resource monitoring and throttling
  - Multi-tenant execution environments

  ## Usage

  Start a realtime execution:

      {:ok, engine} = Dspy.RealtimeExecutionEngine.start_link(
        examples: [example1, example2], 
        realtime: true,
        interval_ms: 5000,
        stream_results: true
      )
      
  Monitor progress:

      metrics = Dspy.RealtimeExecutionEngine.get_metrics(engine)
      
  Stream results:

      Dspy.RealtimeExecutionEngine.subscribe_to_results(engine, self())
  """

  use GenServer
  require Logger

  alias Dspy.{Example, Prediction}

  @type execution_config :: %{
          examples: [Example.t()],
          realtime: boolean(),
          interval_ms: pos_integer(),
          stream_results: boolean(),
          max_concurrent: pos_integer(),
          auto_scaling: boolean(),
          performance_threshold: float(),
          resource_limits: map()
        }

  @type execution_state :: %{
          config: execution_config(),
          status: :running | :paused | :stopped | :error,
          current_cycle: pos_integer(),
          results: [map()],
          metrics: map(),
          subscribers: [pid()],
          last_execution: DateTime.t(),
          performance_history: [float()],
          resource_usage: map(),
          error_count: pos_integer()
        }

  defstruct [
    :config,
    :status,
    :current_cycle,
    :results,
    :metrics,
    :subscribers,
    :last_execution,
    :performance_history,
    :resource_usage,
    :error_count,
    :timer_ref
  ]

  @default_config %{
    realtime: false,
    interval_ms: 10_000,
    stream_results: false,
    max_concurrent: 4,
    auto_scaling: true,
    performance_threshold: 0.8,
    resource_limits: %{
      max_memory_mb: 1000,
      max_cpu_percent: 80,
      max_execution_time_ms: 300_000
    }
  }

  ## Public API

  @doc """
  Start a realtime execution engine.

  ## Options

  - `:examples` - List of DSPy examples to execute continuously
  - `:realtime` - Enable realtime continuous execution (default: false)
  - `:interval_ms` - Execution interval in milliseconds (default: 10000)
  - `:stream_results` - Stream results to subscribers (default: false)
  - `:max_concurrent` - Maximum concurrent executions (default: 4)
  - `:auto_scaling` - Enable automatic scaling based on performance (default: true)
  - `:performance_threshold` - Performance threshold for scaling (default: 0.8)
  """
  @spec start_link(keyword()) :: {:ok, pid()} | {:error, term()}
  def start_link(opts \\ []) do
    config = build_config(opts)
    GenServer.start_link(__MODULE__, config, name: __MODULE__)
  end

  @doc """
  Execute examples with optional realtime mode.

  If realtime is enabled, this will start continuous execution.
  Otherwise, it performs a single execution cycle.
  """
  @spec execute(pid(), [Example.t()], keyword()) :: {:ok, [Prediction.t()]} | {:error, term()}
  def execute(engine, examples, opts \\ []) do
    GenServer.call(engine, {:execute, examples, opts})
  end

  @doc """
  Start realtime continuous execution.
  """
  @spec start_realtime(pid()) :: :ok | {:error, term()}
  def start_realtime(engine) do
    GenServer.call(engine, :start_realtime)
  end

  @doc """
  Stop realtime execution.
  """
  @spec stop_realtime(pid()) :: :ok
  def stop_realtime(engine) do
    GenServer.call(engine, :stop_realtime)
  end

  @doc """
  Pause realtime execution (can be resumed).
  """
  @spec pause_realtime(pid()) :: :ok
  def pause_realtime(engine) do
    GenServer.call(engine, :pause_realtime)
  end

  @doc """
  Resume paused realtime execution.
  """
  @spec resume_realtime(pid()) :: :ok
  def resume_realtime(engine) do
    GenServer.call(engine, :resume_realtime)
  end

  @doc """
  Subscribe to realtime result streams.
  """
  @spec subscribe_to_results(pid(), pid()) :: :ok
  def subscribe_to_results(engine, subscriber_pid) do
    GenServer.call(engine, {:subscribe, subscriber_pid})
  end

  @doc """
  Unsubscribe from result streams.
  """
  @spec unsubscribe_from_results(pid(), pid()) :: :ok
  def unsubscribe_from_results(engine, subscriber_pid) do
    GenServer.call(engine, {:unsubscribe, subscriber_pid})
  end

  @doc """
  Get current execution metrics and performance data.
  """
  @spec get_metrics(pid()) :: map()
  def get_metrics(engine) do
    GenServer.call(engine, :get_metrics)
  end

  @doc """
  Get current execution status.
  """
  @spec get_status(pid()) :: map()
  def get_status(engine) do
    GenServer.call(engine, :get_status)
  end

  @doc """
  Update execution configuration dynamically.
  """
  @spec update_config(pid(), map()) :: :ok | {:error, term()}
  def update_config(engine, new_config) do
    GenServer.call(engine, {:update_config, new_config})
  end

  ## GenServer Implementation

  @impl true
  def init(config) do
    state = %__MODULE__{
      config: config,
      status: :stopped,
      current_cycle: 0,
      results: [],
      metrics: initialize_metrics(),
      subscribers: [],
      last_execution: nil,
      performance_history: [],
      resource_usage: %{},
      error_count: 0,
      timer_ref: nil
    }

    Logger.info("Realtime execution engine started with config: #{inspect(config)}")

    # Start realtime execution if configured
    if config.realtime do
      {:ok, schedule_next_execution(state)}
    else
      {:ok, state}
    end
  end

  @impl true
  def handle_call({:execute, examples, opts}, _from, state) do
    config = Map.merge(state.config, Map.new(opts))

    case execute_examples(examples, config) do
      {:ok, results} ->
        updated_state = %{
          state
          | results: state.results ++ results,
            current_cycle: state.current_cycle + 1,
            last_execution: DateTime.utc_now(),
            metrics: update_metrics(state.metrics, results)
        }

        if config.stream_results do
          broadcast_results(updated_state.subscribers, results)
        end

        {:reply, {:ok, results}, updated_state}

      {:error, reason} ->
        updated_state = %{state | error_count: state.error_count + 1}
        {:reply, {:error, reason}, updated_state}
    end
  end

  @impl true
  def handle_call(:start_realtime, _from, state) do
    if state.status == :running do
      {:reply, {:error, :already_running}, state}
    else
      new_state =
        %{state | status: :running}
        |> schedule_next_execution()

      Logger.info("Started realtime execution with interval: #{state.config.interval_ms}ms")
      {:reply, :ok, new_state}
    end
  end

  @impl true
  def handle_call(:stop_realtime, _from, state) do
    if state.timer_ref do
      Process.cancel_timer(state.timer_ref)
    end

    new_state = %{state | status: :stopped, timer_ref: nil}
    Logger.info("Stopped realtime execution")
    {:reply, :ok, new_state}
  end

  @impl true
  def handle_call(:pause_realtime, _from, state) do
    if state.timer_ref do
      Process.cancel_timer(state.timer_ref)
    end

    new_state = %{state | status: :paused, timer_ref: nil}
    Logger.info("Paused realtime execution")
    {:reply, :ok, new_state}
  end

  @impl true
  def handle_call(:resume_realtime, _from, state) do
    if state.status == :paused do
      new_state =
        %{state | status: :running}
        |> schedule_next_execution()

      Logger.info("Resumed realtime execution")
      {:reply, :ok, new_state}
    else
      {:reply, {:error, :not_paused}, state}
    end
  end

  @impl true
  def handle_call({:subscribe, subscriber_pid}, _from, state) do
    if subscriber_pid in state.subscribers do
      {:reply, :ok, state}
    else
      Process.monitor(subscriber_pid)
      new_state = %{state | subscribers: [subscriber_pid | state.subscribers]}
      Logger.debug("Added subscriber: #{inspect(subscriber_pid)}")
      {:reply, :ok, new_state}
    end
  end

  @impl true
  def handle_call({:unsubscribe, subscriber_pid}, _from, state) do
    new_subscribers = List.delete(state.subscribers, subscriber_pid)
    new_state = %{state | subscribers: new_subscribers}
    Logger.debug("Removed subscriber: #{inspect(subscriber_pid)}")
    {:reply, :ok, new_state}
  end

  @impl true
  def handle_call(:get_metrics, _from, state) do
    metrics = compile_current_metrics(state)
    {:reply, metrics, state}
  end

  @impl true
  def handle_call(:get_status, _from, state) do
    status = %{
      status: state.status,
      current_cycle: state.current_cycle,
      last_execution: state.last_execution,
      subscriber_count: length(state.subscribers),
      error_count: state.error_count,
      next_execution: calculate_next_execution_time(state)
    }

    {:reply, status, state}
  end

  @impl true
  def handle_call({:update_config, new_config}, _from, state) do
    try do
      updated_config = Map.merge(state.config, new_config)
      new_state = %{state | config: updated_config}

      # Reschedule if interval changed and we're running
      new_state =
        if state.status == :running and Map.has_key?(new_config, :interval_ms) do
          if state.timer_ref, do: Process.cancel_timer(state.timer_ref)
          schedule_next_execution(new_state)
        else
          new_state
        end

      Logger.info("Updated execution config: #{inspect(new_config)}")
      {:reply, :ok, new_state}
    rescue
      error ->
        Logger.error("Failed to update config: #{inspect(error)}")
        {:reply, {:error, error}, state}
    end
  end

  @impl true
  def handle_info(:execute_cycle, state) do
    if state.status == :running and state.config.examples do
      start_time = System.monotonic_time(:millisecond)

      case execute_examples(state.config.examples, state.config) do
        {:ok, results} ->
          execution_time = System.monotonic_time(:millisecond) - start_time
          performance_score = calculate_performance_score(execution_time, results)

          updated_state = %{
            state
            | results: limit_results(state.results ++ results, 1000),
              current_cycle: state.current_cycle + 1,
              last_execution: DateTime.utc_now(),
              metrics: update_metrics(state.metrics, results, execution_time),
              performance_history:
                limit_performance_history([performance_score | state.performance_history], 100),
              resource_usage: update_resource_usage(state.resource_usage)
          }

          # Stream results to subscribers
          if state.config.stream_results and length(state.subscribers) > 0 do
            broadcast_results(state.subscribers, results)
          end

          # Auto-scaling based on performance
          updated_state =
            if state.config.auto_scaling do
              apply_auto_scaling(updated_state, performance_score)
            else
              updated_state
            end

          # Schedule next execution
          new_state = schedule_next_execution(updated_state)
          {:noreply, new_state}

        {:error, reason} ->
          Logger.error("Execution cycle failed: #{inspect(reason)}")

          updated_state = %{
            state
            | error_count: state.error_count + 1,
              metrics: update_error_metrics(state.metrics, reason)
          }

          # Continue execution unless too many errors
          new_state =
            if updated_state.error_count < 10 do
              schedule_next_execution(updated_state)
            else
              Logger.error("Too many errors, stopping realtime execution")
              %{updated_state | status: :error}
            end

          {:noreply, new_state}
      end
    else
      {:noreply, state}
    end
  end

  @impl true
  def handle_info({:DOWN, _ref, :process, pid, _reason}, state) do
    # Remove dead subscriber
    new_subscribers = List.delete(state.subscribers, pid)
    new_state = %{state | subscribers: new_subscribers}
    Logger.debug("Removed dead subscriber: #{inspect(pid)}")
    {:noreply, new_state}
  end

  @impl true
  def handle_info(msg, state) do
    Logger.warning("Received unexpected message: #{inspect(msg)}")
    {:noreply, state}
  end

  ## Private Functions

  defp build_config(opts) do
    examples = Keyword.get(opts, :examples, [])
    config_overrides = Map.new(Keyword.delete(opts, :examples))

    @default_config
    |> Map.merge(config_overrides)
    |> Map.put(:examples, examples)
  end

  defp execute_examples(examples, config) do
    try do
      # Execute examples concurrently up to max_concurrent limit
      examples
      |> Enum.chunk_every(config.max_concurrent)
      |> Enum.reduce([], fn chunk, acc ->
        chunk_results =
          chunk
          |> Task.async_stream(
            fn example ->
              execute_single_example(example, config)
            end,
            timeout: config.resource_limits.max_execution_time_ms
          )
          |> Enum.map(fn
            {:ok, result} -> result
            {:exit, reason} -> {:error, {:execution_timeout, reason}}
          end)

        acc ++ chunk_results
      end)
      |> Enum.split_with(fn
        {:ok, _} -> true
        _ -> false
      end)
      |> case do
        {successes, []} ->
          results = Enum.map(successes, fn {:ok, result} -> result end)
          {:ok, results}

        {successes, errors} ->
          Logger.warning(
            "Some executions failed: #{length(errors)} errors, #{length(successes)} successes"
          )

          results = Enum.map(successes, fn {:ok, result} -> result end)
          {:ok, results}
      end
    rescue
      error ->
        Logger.error("Failed to execute examples: #{inspect(error)}")
        {:error, error}
    end
  end

  defp execute_single_example(example, _config) do
    try do
      # Create a basic predict module for the example
      signature = create_signature_from_example(example)
      predict_module = Dspy.Predict.new(signature)

      # Extract inputs from example
      inputs = extract_inputs_from_example(example)

      # Execute prediction
      case Dspy.Module.forward(predict_module, inputs) do
        {:ok, prediction} ->
          result = %{
            example_id: Map.get(example.attrs, :id, generate_id()),
            inputs: inputs,
            prediction: prediction,
            timestamp: DateTime.utc_now(),
            # Simulated for now
            execution_time_ms: :rand.uniform(1000) + 100,
            success: true
          }

          {:ok, result}

        {:error, reason} ->
          {:error, {:prediction_failed, reason}}
      end
    rescue
      error ->
        {:error, {:execution_error, error}}
    end
  end

  defp create_signature_from_example(_example) do
    # Create a basic question-answer signature for the example
    Dspy.Signature.define("question -> answer")
  end

  defp extract_inputs_from_example(example) do
    attrs = example.attrs

    # Extract question or use a default
    question = Map.get(attrs, :question, Map.get(attrs, :input, "Default question"))

    %{question: question}
  end

  defp schedule_next_execution(state) do
    if state.status == :running do
      timer_ref = Process.send_after(self(), :execute_cycle, state.config.interval_ms)
      %{state | timer_ref: timer_ref}
    else
      state
    end
  end

  defp broadcast_results(subscribers, results) do
    message = {:realtime_results, results, DateTime.utc_now()}

    Enum.each(subscribers, fn subscriber ->
      try do
        send(subscriber, message)
      rescue
        # Ignore errors for dead processes
        _ -> :ok
      end
    end)
  end

  defp initialize_metrics() do
    %{
      total_executions: 0,
      total_examples: 0,
      success_rate: 0.0,
      average_execution_time: 0.0,
      throughput_per_second: 0.0,
      error_rate: 0.0,
      performance_trend: :stable,
      resource_utilization: %{
        memory_usage_mb: 0,
        cpu_usage_percent: 0
      }
    }
  end

  defp update_metrics(metrics, results, execution_time \\ nil) do
    successful_results = Enum.filter(results, & &1.success)

    %{
      metrics
      | total_executions: metrics.total_executions + 1,
        total_examples: metrics.total_examples + length(results),
        success_rate: calculate_success_rate(metrics, successful_results, results),
        average_execution_time: update_average_execution_time(metrics, execution_time),
        throughput_per_second: calculate_throughput(metrics, length(results)),
        error_rate: calculate_error_rate(metrics, results)
    }
  end

  defp update_error_metrics(metrics, _reason) do
    %{metrics | error_rate: metrics.error_rate + 0.01}
  end

  defp calculate_success_rate(metrics, successful_results, all_results) do
    if metrics.total_examples == 0 do
      0.0
    else
      total_successful =
        metrics.total_examples * metrics.success_rate + length(successful_results)

      total_examples = metrics.total_examples + length(all_results)
      total_successful / total_examples
    end
  end

  defp update_average_execution_time(metrics, nil), do: metrics.average_execution_time

  defp update_average_execution_time(metrics, execution_time) do
    if metrics.total_executions == 1 do
      execution_time
    else
      (metrics.average_execution_time * (metrics.total_executions - 1) + execution_time) /
        metrics.total_executions
    end
  end

  defp calculate_throughput(metrics, result_count) do
    if metrics.total_executions == 0 do
      0.0
    else
      # Simplified throughput calculation
      result_count / (metrics.average_execution_time / 1000)
    end
  end

  defp calculate_error_rate(_metrics, results) do
    if length(results) == 0 do
      0.0
    else
      error_count = Enum.count(results, fn result -> not result.success end)
      error_count / length(results)
    end
  end

  defp calculate_performance_score(execution_time, results) do
    # Higher score is better
    base_score = 1.0

    # Penalize long execution times
    time_penalty = min(execution_time / 10000, 0.5)

    # Reward successful results
    success_rate = Enum.count(results, & &1.success) / max(length(results), 1)

    max(base_score - time_penalty + success_rate * 0.5, 0.0)
  end

  defp apply_auto_scaling(state, performance_score) do
    cond do
      performance_score < state.config.performance_threshold and state.config.interval_ms > 1000 ->
        # Slow down if performance is poor
        new_interval = min(state.config.interval_ms * 1.5, 60000)
        new_config = %{state.config | interval_ms: round(new_interval)}

        Logger.info(
          "Auto-scaling: increased interval to #{new_config.interval_ms}ms due to poor performance"
        )

        %{state | config: new_config}

      performance_score > 0.9 and state.config.interval_ms > 5000 ->
        # Speed up if performance is excellent
        new_interval = max(state.config.interval_ms * 0.8, 1000)
        new_config = %{state.config | interval_ms: round(new_interval)}

        Logger.info(
          "Auto-scaling: decreased interval to #{new_config.interval_ms}ms due to excellent performance"
        )

        %{state | config: new_config}

      true ->
        state
    end
  end

  defp compile_current_metrics(state) do
    current_metrics =
      Map.merge(state.metrics, %{
        current_cycle: state.current_cycle,
        status: state.status,
        subscriber_count: length(state.subscribers),
        error_count: state.error_count,
        last_execution: state.last_execution,
        performance_history: Enum.take(state.performance_history, 10),
        resource_usage: state.resource_usage,
        config: state.config
      })

    current_metrics
  end

  defp calculate_next_execution_time(state) do
    if state.status == :running and state.last_execution do
      DateTime.add(state.last_execution, state.config.interval_ms, :millisecond)
    else
      nil
    end
  end

  defp update_resource_usage(current_usage) do
    # Simplified resource monitoring
    # MB
    memory_usage = :erlang.memory(:total) / (1024 * 1024)

    %{
      current_usage
      | memory_usage_mb: memory_usage,
        # Simulated
        cpu_usage_percent: :rand.uniform(80) + 10,
        timestamp: DateTime.utc_now()
    }
  end

  defp limit_results(results, max_count) do
    if length(results) > max_count do
      Enum.take(results, -max_count)
    else
      results
    end
  end

  defp limit_performance_history(history, max_count) do
    if length(history) > max_count do
      Enum.take(history, max_count)
    else
      history
    end
  end

  defp generate_id() do
    "exec_" <> (:crypto.strong_rand_bytes(8) |> Base.encode16(case: :lower))
  end
end
