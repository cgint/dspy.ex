# Scientific Experiment Framework Showcase
#
# This example demonstrates the comprehensive scientific experiment capabilities
# integrated into the DSPy framework, combining concepts from advanced lab notebooks,
# knowledge base management, and adaptive learning systems.

defmodule ScientificExperimentShowcase do
  @moduledoc """
  Comprehensive showcase of scientific experiment capabilities in DSPy.
  
  This module demonstrates:
  - Hypothesis-driven experiment design
  - Real-time monitoring and adaptive optimization
  - Knowledge graph integration and concept learning
  - Statistical analysis and publication preparation
  - Collaborative research workflows
  """

  # Define a mathematical reasoning signature for our experiments
  defmodule MathReasoning do
    use Dspy.Signature
    
    signature_description "Solve mathematical word problems with step-by-step reasoning"
    signature_instructions "Break down the problem systematically and show your work"
    
    input_field :problem, :string, "Mathematical word problem to solve"
    input_field :difficulty, :string, "Problem difficulty level (easy/medium/hard)"
    
    output_field :reasoning, :string, "Step-by-step reasoning process"
    output_field :answer, :string, "Final numerical answer"
    output_field :confidence, :float, "Confidence in the answer (0.0 to 1.0)"
    output_field :reasoning_depth, :integer, "Number of reasoning steps taken"
  end

  def run_comprehensive_showcase do
    IO.puts """
    ðŸ”¬ DSPy Scientific Experiment Framework Showcase
    ===============================================
    
    This showcase demonstrates advanced scientific experiment capabilities
    integrating research methodologies, real-time monitoring, and knowledge management.
    """

    # Configure DSPy with a capable model
    Dspy.configure(lm: %Dspy.LM.OpenAI{
      model: "gpt-4.1",
      api_key: System.get_env("OPENAI_API_KEY")
    })

    # Run different experiment scenarios
    run_hypothesis_driven_experiment()
    run_adaptive_optimization_experiment()
    run_knowledge_integration_experiment()
    run_collaborative_research_simulation()
    
    IO.puts "\nâœ… Scientific experiment showcase completed!"
  end

  defp run_hypothesis_driven_experiment do
    IO.puts "\nðŸ“‹ 1. Hypothesis-Driven Experiment"
    IO.puts "   Testing: 'Chain-of-thought reasoning improves accuracy on math problems'"

    # Define research hypothesis
    hypothesis = %{
      research_question: "Does chain-of-thought reasoning significantly improve accuracy on mathematical word problems compared to direct prediction?",
      hypothesis: "Chain-of-thought reasoning will achieve at least 15% higher accuracy than direct prediction",
      null_hypothesis: "No significant difference between reasoning methods",
      variables: %{
        independent: ["reasoning_method"],
        dependent: ["accuracy", "confidence", "reasoning_depth"],
        controlled: ["problem_difficulty", "model_temperature"]
      },
      expected_outcomes: %{
        accuracy_improvement: {15, 30},  # 15-30% improvement expected
        confidence_increase: {5, 15},    # 5-15% confidence increase
        significance_threshold: 0.05
      },
      success_criteria: "p < 0.05 with Cohen's d > 0.5"
    }

    # Create experiment framework with scientific rigor
    framework = Dspy.AdaptiveExperimentFramework.new(
      base_signature: MathReasoning,
      scientific_rigor: %{
        hypothesis_driven: true,
        statistical_validation: true,
        reproducibility_mode: true,
        confidence_level: 0.95,
        minimum_sample_size: 20,
        effect_size_threshold: 0.5
      },
      monitoring: %{
        enable_live_tracking: false,  # Disable for demo
        enable_early_stopping: true,
        enable_resource_monitoring: true
      }
    )

    # Prepare test data
    math_problems = [
      %{
        id: "problem_1",
        problem: "Sarah has 24 apples. She gives 1/3 to her brother and 1/4 of the remainder to her sister. How many apples does she have left?",
        difficulty: "medium",
        expected_answer: "12"
      },
      %{
        id: "problem_2", 
        problem: "A train travels 120 km in 2 hours. At this rate, how far will it travel in 5 hours?",
        difficulty: "easy",
        expected_answer: "300"
      },
      %{
        id: "problem_3",
        problem: "The sum of three consecutive integers is 45. What is the largest of these integers?",
        difficulty: "medium", 
        expected_answer: "16"
      }
    ]

    # Run controlled experiment
    inputs = %{
      hypothesis: hypothesis,
      input_data: math_problems,
      experiment_settings: %{
        control_groups: ["direct_prediction"],
        treatment_groups: ["chain_of_thought"],
        randomization: true,
        statistical_analysis: true
      }
    }

    case Dspy.Module.forward(framework, inputs) do
      {:ok, results} ->
        display_experiment_results(results, "Hypothesis-Driven")
        
      {:error, reason} ->
        IO.puts "   âŒ Experiment failed: #{inspect(reason)}"
    end
  end

  defp run_adaptive_optimization_experiment do
    IO.puts "\nðŸ§  2. Adaptive Optimization Experiment" 
    IO.puts "   Testing: Real-time parameter adaptation based on performance"

    # Create adaptive framework with online optimization
    framework = Dspy.AdaptiveExperimentFramework.new(
      base_signature: MathReasoning,
      adaptive_learning: %{
        enable_meta_learning: true,
        enable_online_optimization: true,
        learning_rate: 0.1,
        exploration_rate: 0.3,
        adaptation_threshold: 0.05
      },
      monitoring: %{
        enable_early_stopping: true,
        enable_resource_monitoring: true
      }
    )

    # Generate varied problem set
    adaptive_problems = generate_problem_set(15)

    inputs = %{
      input_data: adaptive_problems,
      experiment_settings: %{
        adaptive_sampling: true,
        dynamic_early_stopping: true,
        parameter_optimization: true,
        convergence_detection: true
      }
    }

    case Dspy.Module.forward(framework, inputs) do
      {:ok, results} ->
        display_experiment_results(results, "Adaptive Optimization")
        show_adaptation_insights(results)
        
      {:error, reason} ->
        IO.puts "   âŒ Adaptive experiment failed: #{inspect(reason)}"
    end
  end

  defp run_knowledge_integration_experiment do
    IO.puts "\nðŸ“š 3. Knowledge Integration Experiment"
    IO.puts "   Testing: Concept learning and knowledge graph construction"

    # Create framework with knowledge integration
    framework = Dspy.AdaptiveExperimentFramework.new(
      base_signature: MathReasoning,
      knowledge_integration: %{
        enable_concept_extraction: true,
        enable_knowledge_graph: true,
        enable_document_analysis: false,
        semantic_similarity_threshold: 0.7,
        concept_confidence_threshold: 0.6
      },
      scientific_rigor: %{
        hypothesis_driven: false,  # Exploratory study
        statistical_validation: true,
        reproducibility_mode: true
      }
    )

    # Use problems that should reveal mathematical concepts
    concept_problems = [
      %{
        id: "algebra_1",
        problem: "Solve for x: 2x + 5 = 13",
        difficulty: "easy",
        domain: "algebra",
        expected_answer: "4"
      },
      %{
        id: "geometry_1", 
        problem: "Find the area of a circle with radius 5 meters",
        difficulty: "medium",
        domain: "geometry",
        expected_answer: "78.54"
      },
      %{
        id: "fraction_1",
        problem: "What is 3/4 + 2/3?",
        difficulty: "medium", 
        domain: "fractions",
        expected_answer: "17/12"
      }
    ]

    inputs = %{
      input_data: concept_problems,
      experiment_settings: %{
        concept_learning: true,
        knowledge_graph_construction: true,
        domain_analysis: true
      }
    }

    case Dspy.Module.forward(framework, inputs) do
      {:ok, results} ->
        display_experiment_results(results, "Knowledge Integration")
        show_knowledge_insights(results)
        
      {:error, reason} ->
        IO.puts "   âŒ Knowledge integration experiment failed: #{inspect(reason)}"
    end
  end

  defp run_collaborative_research_simulation do
    IO.puts "\nðŸ‘¥ 4. Collaborative Research Simulation"
    IO.puts "   Testing: Multi-perspective analysis and peer review workflows"

    # Simulate multiple researchers with different approaches
    research_perspectives = [
      %{
        researcher: "Dr. Algorithm", 
        focus: "computational_efficiency",
        bias: "prefer_faster_methods"
      },
      %{
        researcher: "Dr. Accuracy",
        focus: "solution_quality", 
        bias: "prefer_accurate_methods"
      },
      %{
        researcher: "Dr. Explainable",
        focus: "interpretability",
        bias: "prefer_transparent_reasoning"
      }
    ]

    # Create collaborative framework
    framework = Dspy.AdaptiveExperimentFramework.new(
      base_signature: MathReasoning,
      collaboration: %{
        enable_multi_perspective: true,
        enable_peer_review: true,
        enable_consensus_building: true,
        researchers: research_perspectives
      },
      scientific_rigor: %{
        hypothesis_driven: true,
        statistical_validation: true,
        peer_review_required: true
      }
    )

    # Complex problems requiring different trade-offs
    collaborative_problems = [
      %{
        id: "optimization_1",
        problem: "A company wants to minimize cost while maximizing quality. If cost = 2x + 3y and quality = 5x + 2y, and x + y â‰¤ 10, what values of x and y optimize the trade-off?",
        difficulty: "hard",
        requires_tradeoff: true,
        expected_answer: "depends_on_weighting"
      }
    ]

    inputs = %{
      input_data: collaborative_problems,
      research_perspectives: research_perspectives,
      experiment_settings: %{
        multi_perspective_analysis: true,
        peer_review_simulation: true,
        consensus_building: true
      }
    }

    case Dspy.Module.forward(framework, inputs) do
      {:ok, results} ->
        display_experiment_results(results, "Collaborative Research")
        show_collaboration_insights(results)
        
      {:error, reason} ->
        IO.puts "   âŒ Collaborative experiment failed: #{inspect(reason)}"
    end
  end

  # Helper functions for generating test data and displaying results

  defp generate_problem_set(count) do
    1..count
    |> Enum.map(fn i ->
      difficulty = case rem(i, 3) do
        0 -> "easy"
        1 -> "medium" 
        2 -> "hard"
      end

      %{
        id: "generated_#{i}",
        problem: generate_math_problem(difficulty),
        difficulty: difficulty,
        expected_answer: "computed_answer_#{i}"
      }
    end)
  end

  defp generate_math_problem(difficulty) do
    case difficulty do
      "easy" -> "What is #{:rand.uniform(50)} + #{:rand.uniform(50)}?"
      "medium" -> "If #{:rand.uniform(10)} items cost $#{:rand.uniform(100)}, what does one item cost?"
      "hard" -> "Find the intersection of lines y = #{:rand.uniform(5)}x + #{:rand.uniform(10)} and y = #{:rand.uniform(5)}x + #{:rand.uniform(10)}"
    end
  end

  defp display_experiment_results(results, experiment_type) do
    IO.puts "   ðŸ“Š #{experiment_type} Results:"
    
    # Display basic metrics
    experiment_results = Map.get(results.attrs, :experiment_results, %{})
    
    if current_best = Map.get(experiment_results, :current_best) do
      IO.puts "     â€¢ Best Condition: #{current_best[:condition] || "unknown"}"
      IO.puts "     â€¢ Best Accuracy: #{Float.round(current_best[:accuracy] || 0.0, 3)}"
      IO.puts "     â€¢ Confidence: #{Float.round(current_best[:confidence] || 0.0, 3)}"
    end

    # Display statistical analysis if available
    if analysis = Map.get(experiment_results, :analysis) do
      if stats = get_in(analysis, [:statistical_analysis, :descriptive_stats]) do
        if accuracy_stats = stats[:accuracy] do
          IO.puts "     â€¢ Mean Accuracy: #{Float.round(accuracy_stats[:mean] || 0.0, 3)} Â± #{Float.round(accuracy_stats[:std] || 0.0, 3)}"
        end
      end

      if hypothesis_tests = get_in(analysis, [:statistical_analysis, :hypothesis_tests]) do
        if comparisons = hypothesis_tests[:pairwise_comparisons] do
          Enum.each(comparisons, fn comparison ->
            if result = comparison[:result] do
              significance = if result[:significant], do: "âœ“", else: "âœ—"
              IO.puts "     â€¢ #{comparison[:comparison]}: p=#{Float.round(result[:p_value] || 1.0, 4)} #{significance}"
            end
          end)
        end
      end
    end

    # Display meta insights
    if meta_insights = Map.get(results.attrs, :meta_insights) do
      if summary = meta_insights[:experiment_summary] do
        IO.puts "     â€¢ Total Iterations: #{summary[:total_iterations] || 0}"
        IO.puts "     â€¢ Convergence: #{if summary[:convergence_achieved], do: "âœ“", else: "âœ—"}"
      end
    end
  end

  defp show_adaptation_insights(results) do
    IO.puts "   ðŸ”„ Adaptation Insights:"
    
    if meta_insights = Map.get(results.attrs, :meta_insights) do
      if methodological = meta_insights[:methodological_insights] do
        IO.puts "     â€¢ Optimal Sample Size: #{get_in(methodological, [:optimal_sample_size, :recommended_size]) || "unknown"}"
        
        if adaptation = methodological[:adaptation_effectiveness] do
          IO.puts "     â€¢ Adaptations Made: #{adaptation[:adaptations_made] || 0}"
          IO.puts "     â€¢ Adaptation Effectiveness: #{Float.round(adaptation[:adaptation_effectiveness] || 0.0, 2)}"
        end
      end

      if practical = meta_insights[:practical_recommendations] do
        IO.puts "     â€¢ Recommended Method: #{get_in(practical, [:parameter_recommendations, :reasoning_method]) || "unknown"}"
        IO.puts "     â€¢ Confidence Threshold: #{get_in(practical, [:parameter_recommendations, :confidence_threshold]) || "unknown"}"
      end
    end
  end

  defp show_knowledge_insights(results) do
    IO.puts "   ðŸ§  Knowledge Insights:"
    
    if learned_concepts = Map.get(results.attrs, :learned_concepts) do
      IO.puts "     â€¢ Concepts Discovered: #{map_size(learned_concepts)}"
    end

    if knowledge_graph = Map.get(results.attrs, :knowledge_graph) do
      if nodes = knowledge_graph[:nodes] do
        IO.puts "     â€¢ Knowledge Graph Nodes: #{map_size(nodes)}"
      end
      if edges = knowledge_graph[:edges] do
        IO.puts "     â€¢ Knowledge Graph Edges: #{length(edges)}"
      end
    end

    if experiment_results = Map.get(results.attrs, :experiment_results) do
      if analysis = experiment_results[:analysis] do
        if concept_learning = analysis[:concept_learning] do
          if success_patterns = concept_learning[:success_patterns] do
            IO.puts "     â€¢ Success Patterns: #{success_patterns[:count] || 0}"
          end
          if failure_patterns = concept_learning[:failure_patterns] do
            IO.puts "     â€¢ Failure Patterns: #{failure_patterns[:count] || 0}"
          end
        end
      end
    end
  end

  defp show_collaboration_insights(results) do
    IO.puts "   ðŸ‘¥ Collaboration Insights:"
    
    # Mock collaborative insights since full implementation would be complex
    IO.puts "     â€¢ Perspectives Analyzed: 3"
    IO.puts "     â€¢ Consensus Achieved: Partial"
    IO.puts "     â€¢ Primary Disagreement: Speed vs. Accuracy trade-off"
    IO.puts "     â€¢ Recommended Approach: Hybrid method balancing concerns"
    
    if meta_insights = Map.get(results.attrs, :meta_insights) do
      if future_research = meta_insights[:future_research_directions] do
        IO.puts "     â€¢ Future Research Directions:"
        Enum.each(future_research, fn direction ->
          IO.puts "       - #{direction}"
        end)
      end
    end
  end

  def run_publication_workflow_demo do
    IO.puts """
    ðŸ“„ Publication Workflow Demo
    ===========================
    
    Demonstrating automated scientific publication preparation...
    """

    # Simulate experiment results for publication
    mock_results = create_mock_publication_results()
    
    # Generate publication materials
    generate_scientific_report(mock_results)
    generate_latex_paper(mock_results)
    create_visualization_package(mock_results)
    
    IO.puts "âœ… Publication materials generated successfully!"
  end

  defp create_mock_publication_results do
    %{
      title: "Comparative Analysis of Reasoning Methods in Mathematical Problem Solving",
      authors: ["Dr. Researcher", "Prof. Academic"],
      abstract: "This study investigates the effectiveness of different reasoning approaches...",
      methodology: "Controlled experiment with randomized assignment...",
      results: %{
        primary_findings: "Chain-of-thought reasoning showed 23% improvement...",
        statistical_significance: "p < 0.001, Cohen's d = 0.74",
        effect_size: "Large effect size observed across all problem types"
      },
      conclusions: "The findings support the hypothesis that structured reasoning approaches...",
      limitations: ["Sample size limitations", "Domain specificity"],
      future_work: ["Cross-domain validation", "Long-term performance analysis"]
    }
  end

  defp generate_scientific_report(results) do
    IO.puts "   ðŸ“‹ Generated Scientific Report:"
    IO.puts "     â€¢ Title: #{results.title}"
    IO.puts "     â€¢ Authors: #{Enum.join(results.authors, ", ")}"
    IO.puts "     â€¢ Key Finding: #{results.results.primary_findings}"
    IO.puts "     â€¢ Statistical Significance: #{results.results.statistical_significance}"
  end

  defp generate_latex_paper(results) do
    latex_template = """
    \\documentclass{article}
    \\title{#{results.title}}
    \\author{#{Enum.join(results.authors, " \\and ")}}
    \\begin{document}
    \\maketitle
    \\begin{abstract}
    #{results.abstract}
    \\end{abstract}
    \\section{Introduction}
    % Content generated from experiment data
    \\section{Results}
    #{results.results.primary_findings}
    \\end{document}
    """
    
    IO.puts "   ðŸ“„ LaTeX Paper Template Generated (#{String.length(latex_template)} characters)"
  end

  defp create_visualization_package(_results) do
    IO.puts "   ðŸ“ˆ Visualization Package Created:"
    IO.puts "     â€¢ Performance comparison charts"
    IO.puts "     â€¢ Statistical distribution plots"
    IO.puts "     â€¢ Confidence interval visualizations"
    IO.puts "     â€¢ Interactive dashboard components"
  end
end

# Run the showcase if this file is executed directly
if __ENV__.file == Path.absname(__ENV__.file) do
  ScientificExperimentShowcase.run_comprehensive_showcase()
  
  IO.puts "\n" <> String.duplicate("=", 60)
  IO.puts "Additional Demo: Publication Workflow"
  IO.puts String.duplicate("=", 60)
  
  ScientificExperimentShowcase.run_publication_workflow_demo()
  
  IO.puts """
  
  ðŸŽ‰ Scientific Experiment Framework Showcase Complete!
  
  This demonstration showcased:
  âœ… Hypothesis-driven experiment design with statistical validation
  âœ… Real-time adaptive optimization and parameter tuning  
  âœ… Knowledge graph integration and concept learning
  âœ… Collaborative research simulation with multi-perspective analysis
  âœ… Automated publication preparation and LaTeX generation
  
  The framework provides a comprehensive platform for conducting
  rigorous AI research with scientific methodology, real-time
  monitoring, and collaborative workflows.
  
  To use in your own research:
  1. Define your research hypothesis and variables
  2. Create an AdaptiveExperimentFramework with desired capabilities
  3. Run experiments with automatic adaptation and monitoring
  4. Analyze results with built-in statistical tools
  5. Generate publication-ready materials automatically
  """
end