defmodule Dspy.Evaluate do
  @moduledoc """
  DSPy Evaluation Framework for measuring program performance.

  Provides comprehensive evaluation capabilities including:
  - Single program evaluation
  - Cross-validation
  - Batch evaluation
  - Performance tracking and comparison

  ## Usage

      # Basic evaluation
      result = Dspy.Evaluate.evaluate(program, testset, metric_fn)
      
      # Cross-validation
      cv_result = Dspy.Evaluate.cross_validate(program, dataset, metric_fn, k: 5)
      
      # Batch evaluation with multiple metrics
      batch_result = Dspy.Evaluate.batch_evaluate(programs, testset, metrics)

  """

  alias Dspy.{Example, Module}

  @type evaluation_result :: %{
          scores: list(number()),
          mean: number(),
          std: number(),
          min: number(),
          max: number(),
          count: pos_integer(),
          successes: pos_integer(),
          failures: pos_integer()
        }

  @type cross_validation_result :: %{
          fold_scores: list(number()),
          mean_score: number(),
          std_score: number(),
          fold_results: list(evaluation_result())
        }

  @doc """
  Evaluate a program on a test set using the given metric.

  ## Parameters

  - `program` - DSPy program module to evaluate
  - `testset` - List of test examples
  - `metric_fn` - Metric function (example, prediction -> score)
  - `opts` - Options: num_threads, progress, return_all

  ## Returns

  `evaluation_result()` with detailed statistics

  ## Examples

      result = Dspy.Evaluate.evaluate(program, testset, &Dspy.Metrics.exact_match/2)
      # %{mean: 0.85, std: 0.36, count: 100, ...}

  """
  @spec evaluate(module(), list(Example.t()), function(), keyword()) :: evaluation_result()
  def evaluate(program, testset, metric_fn, opts \\ []) do
    num_threads = Keyword.get(opts, :num_threads, System.schedulers_online())
    show_progress = Keyword.get(opts, :progress, false)
    return_all = Keyword.get(opts, :return_all, false)

    if show_progress do
      IO.puts("Evaluating #{length(testset)} examples...")
    end

    # Chunk testset for parallel processing
    chunk_size = max(1, div(length(testset), num_threads))
    chunks = Enum.chunk_every(testset, chunk_size)

    # Process chunks in parallel
    results =
      chunks
      |> Task.async_stream(
        fn chunk ->
          evaluate_chunk(program, chunk, metric_fn, show_progress)
        end,
        max_concurrency: num_threads,
        timeout: :infinity
      )
      |> Enum.flat_map(fn {:ok, chunk_results} -> chunk_results end)

    # Separate scores and predictions
    {scores, predictions} = Enum.unzip(results)
    valid_scores = Enum.filter(scores, &is_number/1)

    # Calculate statistics
    mean_score =
      if length(valid_scores) > 0, do: Enum.sum(valid_scores) / length(valid_scores), else: 0.0

    std_score = calculate_std(valid_scores, mean_score)

    result = %{
      scores: if(return_all, do: scores, else: valid_scores),
      predictions: if(return_all, do: predictions, else: []),
      mean: mean_score,
      std: std_score,
      min: if(length(valid_scores) > 0, do: Enum.min(valid_scores), else: 0),
      max: if(length(valid_scores) > 0, do: Enum.max(valid_scores), else: 0),
      count: length(testset),
      successes: length(valid_scores),
      failures: length(testset) - length(valid_scores)
    }

    if show_progress do
      IO.puts(
        "Evaluation complete: #{Float.round(result.mean, 3)} Â± #{Float.round(result.std, 3)}"
      )
    end

    result
  end

  @doc """
  Perform k-fold cross-validation on a dataset.

  ## Parameters

  - `program` - DSPy program to evaluate
  - `dataset` - Full dataset to split into folds
  - `metric_fn` - Metric function
  - `opts` - Options: k (default 5), shuffle, seed

  ## Returns

  `cross_validation_result()` with fold-by-fold results

  """
  @spec cross_validate(module(), list(Example.t()), function(), keyword()) ::
          cross_validation_result()
  def cross_validate(program, dataset, metric_fn, opts \\ []) do
    k = Keyword.get(opts, :k, 5)
    shuffle = Keyword.get(opts, :shuffle, true)
    seed = Keyword.get(opts, :seed, :os.system_time(:microsecond))

    # Shuffle dataset if requested
    shuffled_dataset =
      if shuffle do
        :rand.seed(:exsss, {seed, seed + 1, seed + 2})
        Enum.shuffle(dataset)
      else
        dataset
      end

    # Create folds
    fold_size = div(length(shuffled_dataset), k)

    folds =
      shuffled_dataset
      |> Enum.chunk_every(fold_size)
      |> Enum.take(k)

    # Evaluate each fold
    fold_results =
      folds
      |> Enum.with_index()
      |> Enum.map(fn {test_fold, idx} ->
        train_folds =
          folds
          |> Enum.with_index()
          |> Enum.reject(fn {_, i} -> i == idx end)
          |> Enum.map(&elem(&1, 0))

        _train_set = List.flatten(train_folds)

        IO.puts("Cross-validation fold #{idx + 1}/#{k}")
        evaluate(program, test_fold, metric_fn, progress: false)
      end)

    # Calculate overall statistics
    fold_scores = Enum.map(fold_results, & &1.mean)
    mean_score = Enum.sum(fold_scores) / length(fold_scores)
    std_score = calculate_std(fold_scores, mean_score)

    %{
      fold_scores: fold_scores,
      mean_score: mean_score,
      std_score: std_score,
      fold_results: fold_results
    }
  end

  @doc """
  Evaluate multiple programs on the same test set.

  ## Parameters

  - `programs` - List of {name, program} tuples
  - `testset` - Test examples
  - `metrics` - List of {name, metric_fn} tuples
  - `opts` - Options

  ## Returns

  Map of program_name -> metric_name -> result

  """
  @spec batch_evaluate(
          list({String.t(), module()}),
          list(Example.t()),
          list({String.t(), function()}),
          keyword()
        ) :: map()
  def batch_evaluate(programs, testset, metrics, opts \\ []) do
    for {prog_name, program} <- programs, into: %{} do
      metric_results =
        for {metric_name, metric_fn} <- metrics, into: %{} do
          {metric_name, evaluate(program, testset, metric_fn, opts)}
        end

      {prog_name, metric_results}
    end
  end

  @doc """
  Compare two evaluation results and return improvement statistics.

  ## Parameters

  - `baseline_result` - Baseline evaluation result
  - `optimized_result` - Optimized evaluation result

  ## Returns

  Map with improvement statistics

  """
  @spec compare_results(evaluation_result(), evaluation_result()) :: map()
  def compare_results(baseline, optimized) do
    improvement = optimized.mean - baseline.mean
    relative_improvement = if baseline.mean != 0, do: improvement / baseline.mean, else: 0

    %{
      baseline_score: baseline.mean,
      optimized_score: optimized.mean,
      absolute_improvement: improvement,
      relative_improvement: relative_improvement,
      improvement_pct: relative_improvement * 100,
      statistical_significance: statistical_significance_test(baseline, optimized)
    }
  end

  # Private functions

  defp evaluate_chunk(program, examples, metric_fn, _show_progress) do
    examples
    |> Enum.map(fn example ->
      try do
        # Run program on example
        case Module.forward(program, example.attrs) do
          {:ok, prediction} ->
            score = Dspy.Teleprompt.run_metric(metric_fn, example, prediction)
            {score, prediction}

          {:error, _reason} ->
            {:error, nil}
        end
      rescue
        _ -> {:error, nil}
      end
    end)
  end

  defp calculate_std([], _mean), do: 0.0
  defp calculate_std([_], _mean), do: 0.0

  defp calculate_std(values, mean) do
    variance =
      values
      |> Enum.map(&:math.pow(&1 - mean, 2))
      |> Enum.sum()
      |> Kernel./(length(values) - 1)

    :math.sqrt(variance)
  end

  defp statistical_significance_test(baseline, optimized) do
    # Simple t-test approximation
    if baseline.count > 10 and optimized.count > 10 do
      pooled_std = :math.sqrt((baseline.std * baseline.std + optimized.std * optimized.std) / 2)

      if pooled_std > 0 do
        t_stat =
          abs(optimized.mean - baseline.mean) /
            (pooled_std * :math.sqrt(2 / min(baseline.count, optimized.count)))

        cond do
          # p < 0.01
          t_stat > 2.58 -> :highly_significant
          # p < 0.05
          t_stat > 1.96 -> :significant
          # p < 0.10
          t_stat > 1.65 -> :marginally_significant
          true -> :not_significant
        end
      else
        :insufficient_data
      end
    else
      :insufficient_data
    end
  end
end
